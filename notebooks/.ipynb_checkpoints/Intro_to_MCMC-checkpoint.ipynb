{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03bbdac",
   "metadata": {},
   "source": [
    "# Computational Methods in Precision Cosmology\n",
    "\n",
    "Produced by __Elsa Teixeira__ and __William Giarè__\n",
    "\n",
    "[orginal version: April 2023, revised version: April 2024]\n",
    "\n",
    "***\n",
    "\n",
    "In this section we aim to introduce the main statistical methods employed to produce observational constraints on cosmological parameters.\n",
    "<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2647617",
   "metadata": {},
   "source": [
    "The unprecedented surge in **precision probes** of the Universe has made it possible to constrain the parameters of the standard cosmological model, with outstanding accuracy, bringing to light a new crisis in the field, with theoretical problems and **observational tensions** between multiple probes lacking a reasonable resolution. Cosmology is characterised by a strong synergy between the **theoretical formulation** and **numerical analysis** of models and the planning of observational probes and collection and treatment of relevant data. At this critical moment for research in cosmology, establishing links between the studies and tests of fundamental physics, the data science strategies being developed and current and future observational methods is essential. Testing the standard model of cosmology is currently a significant effort in the community, implying that a collective and interdisciplinary approach is crucial to achieving this common goal.\n",
    "\n",
    "With new theories being proposed everyday, data analysis in cosmology has become **essential to test, and even falsify, their statistical support**, in the journey to find a framework able to fit the multitude of data and address the tensions, with many subtelties at play (theoretical biases, model independence of the analysis, overfitting due to excess of parameters, degeneracies in the parameter space, etc.). Ultimately, developing and improving current statistical methods and techniques making the link between precision data and accurate theoretical predictions is of paramount importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6484a276",
   "metadata": {},
   "source": [
    "## 1. What is Bayesian Inference? <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "Due to its nature, analysis problems are inherently _inverse problems_, that is, given some data that has been collected, we wish to **infer something about the physical process that produced the data itself**. A typical example is fitting the _Planck_ CMB data to the $\\Lambda$CDM model:\n",
    "\n",
    "![alt text](./images/planck.png \"Planck_CMB\")\n",
    "\n",
    "\n",
    "Then, given the Planck CMB data, we may want to infer e.g. how much dark matter or baryons are there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50bea4",
   "metadata": {},
   "source": [
    "There are two radically different approaches to statistics:\n",
    "\n",
    "* The **frequentist** or classical notion of probability: related to the **frequency of particular outcomes** for a large (ideally infinite) series of trials. Relies on repeatability of the experiment.\n",
    "\n",
    "* The **Bayesian** outlook: probability is based on the knowledge available prior to the experiment, encoding a **notion of confidence or belief in particular conditions**. Relies on confidence of information relevant for the experiment. \n",
    "\n",
    "\n",
    "Formally, the probability $p(x|y)$ tells us the **degree to which the fact that the logical proposition $y$ is true implies that $x$ is also true**. Therefore, building on the previous example, the Bayesian approach would be to ask: _given_ the Planck CMB data, what is the probability that the density parameter of cold dark matter is found to be between 0.3 and 0.4?\n",
    "\n",
    "The inferential use of the Bayes theorem for a model $M$ with parameters $\\theta$ fitting some data distribution $d$ is expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "p \\left(\\theta \\mid d\\right) = \\frac{p \\left( d \\mid \\theta \\right)\\, p \\left( \\theta \\right)}{p \\left( d \\right)} = \\frac{p \\left( d \\mid \\theta \\right)\\, p \\left( \\theta \\right)}{ \\int \\mathrm{d}\\theta_i p \\left( d \\mid \\theta_i \\right)\\, p \\left( \\theta_i \\right)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The term on the left-hand side, $p \\left(\\theta \\mid d, M \\right)$ is the __posterior__ probability density function (PDF) for $\\theta$ under the model $M$ which we wish to compute. From a Bayesian point of view it expresses the confidence degree in the values of $\\theta$ according to the information given by the data $d$. In practice, it expresses the **probability that the $\\theta$ in the theory $M$ can be explained by the data $d$**.\n",
    "\n",
    "The first term on the right-hand side, $p \\left( d \\mid \\theta \\right) = \\mathcal{L} (\\theta)$ is the __likelihood__ function, corresponding instead to the **probability/likeliness of the observed data given a certain value of the parameter $\\theta$**, given as a function of the parameter itself. Thus, it is the PDF of a given data set $d$ to be explained by the parameters $\\theta$.\n",
    "\n",
    "$p \\left( \\theta \\right)$ is the __prior__ PDF, which encodes the degree of belief in the values of $\\theta$ before we see the data or, in simple terms, the prior **information/knowledge of the parameter before the experiment**. This is a key element for Bayesian inference in physics, usually **assessed from the theory or from past experiments**. More comonly, either flat priors or Gaussian priors are chosen, but there may be situations that require more sophisticated choices. As more data becomes available the prior becomes unimportant for parameter inference and the likelihood dominates.\n",
    "\n",
    "The denominator, $p \\left( d \\right)$ is seen as the condition imposing the normalisation of the posterior probability dependent on the data distribution $d$ known as the __evidence__:\n",
    "\n",
    "\\begin{equation}\n",
    "p \\left( \\theta \\right) = \\int p \\left( d \\mid \\theta \\right) p \\left( \\theta \\right) \\mathrm{d}\\, \\theta\n",
    "\\end{equation}\n",
    "\n",
    "For this reason it can be disregarded in the simplest cases for parameter inference, and only becomes important for the statistical _evidence_ comparison between different models. This model selection creteria relies on the fact that the Bayes’ theorem relates what we learn about $\\theta$ _after_ seeing the data (the posterior) to what we knew about $\\theta$ _before_ looking at the data (the likelihood and the prior). Therefore, in some way it measures how much we have learned or **how much our knowledge has been updated from the prior to the posterior**. \n",
    "\n",
    "The basic principle of bayesian inference is that our knowledge about the quantity we are analysing is **updated in cycles**: the posterior from a previous iteration becomes the prior for the next one. This means that we need to have a starting point, which corresponds exactly to fixing an **initial prior** which must be given as an input and that should be a good representation of the current knowledge about the quantity we wish to analyse.\n",
    "In normal conditions the posterior will converge to an unique and \"true\" result independently of the prior, as long as the prior range includes regions of the parameter space with large likelihoods.\n",
    "\n",
    "Once the posterior distribution has been computed, a lot of information about the parameters of the model $\\{ \\theta_i \\} = \\{ \\theta_1, \\theta_2, ..., \\theta_n \\}$ given the data $d$ can be extracted. First, an estimator for the parameters by maximising the posterior distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial p \\left(\\theta_i \\mid d \\right)}{\\partial \\theta_i} = 0,\\ \\ \\text{for } i=1, ..., n.\n",
    "\\end{equation}\n",
    "\n",
    "If we wish to find the probability of a single chosen parameter $\\theta_i$ we can integrate over the remaining parameters, a technique called **marginalisation**:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\theta_i) = \\int ... \\int p(\\theta_i | d) \\mathrm{d} \\theta_1 ... \\mathrm{d} \\theta_{i-1}\\, \\mathrm{d} \\theta_{i+1} ... \\mathrm{d} \\theta_{n}.\n",
    "\\end{equation}\n",
    "\n",
    "This is especially important in the case where there are multiple _nuisance parameters_, related with the experiment and the method itself, which are not informative for the cosmological analysis.\n",
    "\n",
    "Finally, we can also compute the **confidence level** for the inferred parameters as:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\theta_i) = \\int_{R(r)} p(\\theta_i | d) \\mathrm{d}^n \\theta = r\n",
    "\\end{equation}\n",
    "\n",
    "where $r$ is some fraction linked to the probability that the parameters will be contained in that parameter region. The most commonly studied cases ar $r = 0.683, 0.954, 0.997$, corresponding to the $1\\sigma$, $2\\sigma$ and $3\\sigma$ confidence levels, respectively. \n",
    "\n",
    "\n",
    "If the **PDFs are Gaussian** then **maximising the likelihood** is essentially equivalent to the **least square method of minimising the $\\chi^2$** (best fit of parameters), which corresponds to the the exponent of the Gaussian distribution $$\\mathcal{L} (d|\\theta) \\propto \\exp(- \\chi^2/2).$$\n",
    "Even though this is not the case for all the data, in fact the central limit theorem assures that if we have enough samples ($N$), each with a given PDF and finite variance, then in the limit of very large $N$ ($N \\rightarrow \\infty$), the sum will approach a Gaussian distribution, greatly simpifying the analysis. The maximum likelihood estimation is more general that the minimum $\\chi^2$ and the best-fitting parameters will correspond to $\\nabla_{\\theta} \\mathcal{L} (d|\\theta) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6f599",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo methods <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "\n",
    "Only in exceptionally simple scenarios will the the posterior be an analytic function. Even if that is the case, evaluating it on a grid in parameter space is very **computationally expensive**, even if we have just 2 or 3 parameters. Therefore, we need to find numerical methods that optimise the computation. A standard technique involves taking random steps in parameter space according to some porposed distribution and an acceptance criteria. In this case we will obtain a __chain of samples of the posterior__ with an expected number density proportional to the posterior itself.\n",
    "\n",
    "In broad terms, __Markov Chain Monte Carlo__ (MCMC) methods constitute a category of sampling algorithms to derive the posterior distribution. __Monte Carlo methods__ are a broad class of computational algorithms based on __consecutive random sampling__ to achieve numerical solutions. Therefore, the aim of any MCMC algorithm is to generate a _sequence_ of points in parameter space, a _chain_, with a density proportional to the posterior PDF. More precisely, a __Markov chain__ is defined as a sequence of $m>i$ random variables with probability density proportional to some known function. This is a stochastic method in the sense that the probability of the $(i+1)$th element in the chain depends only on the value of the $i$th element. We say that a Markov chain has **converged when it reaches a stationary state** (does not change for increasing $i$), where successive elements of the chain become samples from the target posterior distribution. The iteration method to generate repeated elements of the chain has a probabilistic nature, measured through a **transition probability** $T(\\theta_i,\\theta_{i+1})$ in parameter space. Having a stochastic nature, this means that in a Markov chain the transition probability satisfies the detailed _balance condition_\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\theta_i|d)T(\\theta_i,\\theta_{i+1}) = p(\\theta_{i+1}|d)T(\\theta_{i+1},\\theta_i)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "which simply means that the ratio of the probabilities for going from $\\theta_i$ to $\\theta_{i+1}$ is inversely proportional to the ratio of the corresponding posterior probabilities. As is the general _motto_ in statistics, the more steps are included, the more closely will the sampled distribution resemble the actual target distribution.\n",
    "\n",
    "In practice, a number of chains are generated, starting from arbitrarily chosen positions in the parameter space which shoud be sufficiently separated from each other. This will generate a _random walk_ following an algorithm that assigns higher _jumping_ probabilities to iterations in which the parameters give a considerable contribution to the distribution.\n",
    "This samples become automatically auto-correlated which leads to the need for employing the Markov chain central limit theorem to properly derive the error in the mean values.\n",
    "\n",
    "### Metropolis-Hastings (MH) algorithm\n",
    "\n",
    "Based on the Metropolis algorithm, arguably the simplest MCMC algorithm, for parameters with a non-symmetrical distribution. Generate states **based on a distribution**, in this case corresponding to the likelihood distribution $P(x)$, with the **aim of approaching a stationary solution with distribution equal to $P(x)$**.\n",
    "\n",
    "__Algorithm:__\n",
    "\n",
    ">1. Start from an __initial point in the parameter space__ $\\theta_0$ randomly selected from the prior density, and __compute its posterior__, that is, $p_0 \\equiv p(\\theta_0|d)$.\n",
    ">\n",
    ">2. __Propose a next candidate iteration__ $\\theta_c$ based on the proposal distribution $q(\\theta_0,\\theta_c)$, which does not necessarily need to satisfy the symmetry condition $q(x, y) = q(y, x)$. The most common and simple example of a symmetric distribution would be a Gaussian of fixed width $\\sigma$ centred around the current point.\n",
    ">\n",
    ">3. In the same way, evaluate the value of the posterior at the candidate point, $p_c = p(\\theta_c|d)$. The __probability of acceptance of the candidate point__ is given as\n",
    ">\n",
    ">\\begin{equation}\n",
    "\\alpha (\\theta_0, \\theta_c) = min \\left( \\frac{p_c}{p_0} \\frac{q(\\theta_c, \\theta_0)}{q(\\theta_0, \\theta_c)} ,1 \\right).\n",
    "\\end{equation}\n",
    ">\n",
    ">The __criteria for acceptance/rejection__ is based on generating a uniform random number $u \\in [0, 1]$ and accepting the candidate sample if $u < \\alpha$, and rejecting if otherwise.\n",
    ">\n",
    ">4. If the candidate point is accepted, it is __added to the chain__ and becomed the new starting point for the next iteration. Otherwise stay at the old point, which is registered again in the chain. __Repeat the procedure__ by starting again from 2.\n",
    "\n",
    "Note how if the distribution is symmetric then the candidate sample is always accepted if it has a larger posterior than the current one, that is if $p_c > p_0$. \n",
    "\n",
    "The normalisation constant given by the evidence becomes irrelevant as it is eliminated in the ratio computed in the acceptance rate. This may be problematic for model comparison. However, it means that __marginalising over the remaining parameters becomes trivial__, as we can simply discard the parameter to be marginalised in each sample.\n",
    "\n",
    "However, this method has some well-known __limitations__. First, __the initial point is chosen arbitrarily__. This means that the starting point in the chain may be significantly far from the peak of the stationary distribution, resulting in a significantly slowing down of the process. This is usually accounted for by discarding some fraction of the initial accumulated chains samples (__burn-in fraction__). \n",
    "Second, __the proposal distribution is also arbitrary__. This means that if the distributions is too large we may overshoot the peak; on the other hand, if it's too small convergence to the stationary distribution may never be achieve. \n",
    "Third, __some previous knowledge about the proposal distribution is needed__, meaning that the results have to be taken with a grain of salt and carefully examined. This also implies that MH is very inefficient if the distribution is multi-modal, in which case other sampling methods may be necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096335d",
   "metadata": {},
   "source": [
    "![Metropolis Hastings algorithm](https://revolution-computing.typepad.com/.a/6a010534b1db25970b019aff4a7bbc970d-800wi)\n",
    "\n",
    "Reference: https://blog.revolutionanalytics.com/2013/09/an-animated-peek-into-the-workings-of-Bayesian-statistics.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f4ee3",
   "metadata": {},
   "source": [
    "## Convergence criteria\n",
    "\n",
    "***\n",
    "\n",
    "One of the key questions in MCMC concerns the appropriate running time for achieving accurate results. The fact is that you can never be completely sure that you have sampled in the full posterior pdf. This is a problem for instance in multimodal scenarios scenarios. The simpler example is to have two modes, that is two separated regions of substantial total probability, in the posterior pdf that are both important but separated by a large region of low probability. A simple sampler mught be able to sample one of the modes really well but the sampler will effectively never find the other mode (i.e. it will take infinite time). Most real problemshave an unknown number of modes and knowing for sure that you have achieved a complete and representative sampling is effectively impossible.\n",
    "The point is that we don’t and can’t require any kind of absolute convergence!\n",
    "\n",
    "In practice we assume that we have sampled for long enough when __any substancial subset of the chain presents practically the same posterior pdf morphology__. Equivalently, we may look for independently and randomly initialised chains that produce posterior pdfs that are substantially the same.\n",
    "\n",
    "In practice we want to quantify the amount of deviation expected between the means and variances, of two disjoint or independent subsets of the chain. We want to find what is called the “integrated autocorrelation time” of the chain. This function measures the similarity between nearby points in the sequence, sufficiently distant such that the points will not have memory or be blind to one another.\n",
    "\n",
    "One simple and generally effective test of convergence is the **Gelman–Rubin diagnostic**, which compares the variance (in individual or in all parameters) within a chain to the variance across chains. This of course requires running multiple chains, and looking at the variance of the parameter away from its mean within each __individual chain__, and comparing it to the variance in the mean of that parameter __across chains__. Exactly what Gelman-Rubin does with these variances is complex, but the important question for convergence is whether, as the chains get longer, these two variances asymptotically reach stable values, and that those two values agree. This is measured as $r-1$ in the example below and an usual criteria is to assume the MCMC has converged if $\\max(r-1) \\approx 10^{-2}$ across all the sampled parameters. The Gelman–Rubin diagnostic is related to the autocorrelation time, since convergence will only be achieved once the chains are much longer than an autocorrelation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66743e",
   "metadata": {},
   "source": [
    "## Proposal Distribution\n",
    "\n",
    "\n",
    "Most MCMC methods require a \"proposal distribution\", which determines the \"length\" of each step of the random-walk through the parameter space. For example, a D-dimensional Gaussian (normal distribution) is often used to draw probabilities for moving from point to point. Even within this simple choice (which is of one function among many possibilities), there are D (D+1)/2 parameters in the D×D symmetric covariance matrix to be set. The problem is that if the the proposal distribution leads to too small steps then almost all steps will be accepted at the cost of taking a long time to move anywhere. On the other hand, if the proposal distribution leads to too large steps then the parameter space will be easily covered but almost no steps will be acceptedand we will tend to end up in much lower probability regions. \n",
    "\n",
    "Intuitively, the optimal step size should be the step size that makes for the shortest autocorrelation time, approximated by the the __acceptance rate__. The golden ratio for the acceptance rate (fraction of accepted moves) is between a quarter and about a half, with the best performance argued for ~0.23 in high dimensional problems). \n",
    "This tuning of the acceptance rate must take place during the __burn-in phase__ (in which some part of the chain will be discard later) of an MCMC run. This is because the access to the past history of the chains is a violation of the \"Markovity\" of the chains, for which each step only depends on the previous state. During this process we aim to track the acceptance ratio, and adjust the proposal distribution variance as you get acceptance ratios that are far from the ideal ratio, with this process being easily automated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae7826",
   "metadata": {},
   "source": [
    "## Model Comparison: Goodness of fit Criteria\n",
    "\n",
    "\n",
    "It is important to highlight the fundamental difference between model fitting and model selection. The process of **model fitting** relies on assuming that a particular model is the true model and extracting the constraints that provide the best possible fit to the available data. On the other hand, in **model selection** we are interested in assessing the level of compatibility of each model with the data. \n",
    "\n",
    "In exceptionally simple cases we can select between models by comparing the value of the maximum likelihood, but this is not valid in general. If model A has more degrees of freedom (i.e., more parameters) than the model B (and the model B can be recovered from model A), than  A will always give an equal or larger maximum likelihood compared to B, irrespectively of the data! This makes sense as when model complexity is increased, then the maximum likelihood value will generally increase, often at the cost of over-fitting the data. \n",
    "\n",
    "This means that accurate model selection should focus on comparing models while accounting for correcting to over-fitting present in models with increased complexity. The key difference is in comparing __posteriors__ instead of __likelihoods__.\n",
    "\n",
    "***\n",
    "\n",
    "Many of the questions we wish to answer in Cosmology go beyond parameter inference and fall on __the model comparison realm__. Examples:\n",
    "\n",
    "* Which is the most probable model of inflation to explain current data?\n",
    "* Can we impose adiabatic initial conditions for the perturbed modes?\n",
    "* Is dark energy dynamical?\n",
    "* Are there signs on non-gaussianities?\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "Simple models are built on the underlying assumption that the observed data should be normally distributed around the corresponding model values up to some standard deviation. This implies that the sum of squares of normalised residuals arounf the best-fit model should follow the $\\chi^2$ distribution: \n",
    "\n",
    "$$\\chi^2 (d;\\theta,M) = \\sum_{n=1}^N \\left[\\frac{y_n - y_M (\\theta)}{\\sigma_{y,n}} \\right]^2,$$\n",
    "\n",
    "where we have assumed that there are $N$ data points $d(n)$ with corresponding errors $\\sigma_n$. In each case the theory predictions can be estimated according to a given set of parameters $\\Theta$. Optimisation of $\\Theta$ implies minimisation of the $\\chi^2$. This statistic is proportional to the negative log likelihood, up to a constant offset, and so for model fitting, minimizing the $\\chi^2$ is equivalent to maximizing the likelihood.\n",
    "\n",
    "The key information is that this statistic replaces the quantity $P(d | M)$ with an easily computable quantity $P(\\chi^2 | M_{\\chi^2})$ (with known probability distribution). We are implicitly replacing the question of how likely is it to observe the data $D$ assuming the $M$ by how likely are we to see the $\\chi^2$ statistic under the best-fit model $M_{\\chi^2}$, and the two likelihoods will be equivalent as long as our assumptions hold (namely that the data is normally distributed).\n",
    "\n",
    "However, in most cases the data are not independent and some given _**error covariance**_ is also needed to compute $\\chi^2$:\n",
    "\n",
    "$$\\chi^2=\\sum_{n=1}^{N}\\left[y_n - y_M (\\theta)\\right]^T \\mathrm{COV}^{-1}\\,\\left[y_n - y_M (\\theta)\\right],$$\n",
    "\n",
    "$\\mathrm{COV}^{-1}$ is the inverse covariance matrix where the diagonal terms are $1/\\sigma_{y,n}^2$.\n",
    "\n",
    "For a simpler analysis we can consider information criteria for approximate model comparison keeping in mind that these make use of considerably strong assumptions about the posterior distribution:\n",
    "\n",
    "* Akaike Information Criterium (AIC): $\\mathrm{AIC} \\equiv \\chi^2_{\\rm eff} + 2k$\n",
    "* Bayesian Information Criterium (BIC): $\\mathrm{BIC} \\equiv \\chi^2_{\\rm eff} + k \\ln (N)$\n",
    "* Deviance Information Criterium (DIC): $\\mathrm{DIC} \\equiv \\chi^2_{\\rm eff} + 2 \\left[ \\bar{\\chi^2_{\\rm eff}} - \\chi^2_{\\rm eff} \\right]$\n",
    "\n",
    "\n",
    "where k is the number of fitted parameters, N is the number of data points, and the terms $\\chi^2_{\\rm eff} \\equiv -2 \\ln (\\mathcal{L}_\\mathrm{max})$ gives $\\chi^2$ of the best-fit, and the upper bar denotes quantities computed at the average of the posterior distribution.\n",
    "\n",
    "In this case the best model must minimise the AIC/BIC/DIC. However, each criteria penalises models differently, with the BIC including a stronger penalty for models with a larger number of free parameters $k$ when the number of data points $N$ is sufficiently large and gives a better approximation to the full Bayesian evidence in the large $N$ limit. On the other hand, by evaluating the difference according to the average of the posterior distributions, the DIC also considers whether information about the parameters is actually gained or not. Because of all these subtelties, it's always preferable to calculate the full Bayesian evidence, although this may be non-trivial for more complex cases, in which the information criteria already provide some useful insight.\n",
    "\n",
    "As an intuitive example, the $\\Delta \\mathrm{AIC} = \\mathrm{AIC}(M_2) - \\mathrm{AIC}(M_1)$ is an approximation to the Bayes factor.\n",
    "It penalises models with more parameters (a correction for the sample size can be included as $\\frac{2k (k+1)}{N-p-1}$, with $N$ being the number of bins). The model with **the smaller value of $\\mathrm{AIC}$ is preferred**, since the likelihood ratio is $e^{\\Delta \\mathrm{AIC}/2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d582fe",
   "metadata": {},
   "source": [
    "## Model Comparison: Bayesian Criteria\n",
    "\n",
    "As we mentioned before, one way to assess preference for different models can be related to the Bayesian evidence or model likelihood:\n",
    "\n",
    "\\begin{equation}\n",
    "p \\left(d | M \\right) = \\int \\mathrm{d}\\theta_i p \\left( d | \\theta_i, M \\right)\\, p \\left( \\theta_i | M \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "which is simply expressed as the integral of the likelihood over the prior range and we have now made the model dependence explicit.\n",
    "\n",
    "Basically we determine the values of the $\\chi^2$ over a grid of $n$ parameters and convert to probability $P(\\theta_1, ..., \\theta_n) \\propto e^{-\\chi^2/2}$. Then we __marginalise to obtain the posterior probability distributions__ for each parameter, $P(\\theta_1), P(\\theta_2),...$ and so on. The integration under these distributions leads to what is referred to as the confidence regions (such as $68\\%$ and $99\\%$). Because this integral is computed over the entire parameter space of the model it can be extremely computationally expensive, especially was the dimension of the model grows beyond 2 or 3.\n",
    "\n",
    "The model's posterior is expressed simply in terms of Bayes' theorem and it's this ratio that quantifies the model comparison:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{p (M_1 | d)}{p (M_2 | d)} = \\frac{p (d | M_1)}{p (d | M_2} \\frac{p (M_1)}{p(M_2)}\n",
    "\\end{equation}\n",
    "\n",
    "which defining the Bayes factor as \n",
    "\n",
    "\\begin{equation}\n",
    "B_{12} \\equiv \\frac{p (d | M_1)}{p (d | M_2)}\n",
    "\\end{equation}\n",
    "\n",
    "is equivalent to the statement _posterior odds = Bayes factor $\\times$ prior odds_.\n",
    "\n",
    "The strength of the evidence is evaluated according to Jeffrey's scale for the favoured model (different definitions in the literature):\n",
    "\n",
    "| $\\mid \\ln{B}\\mid$ | fractional odds | model's probability | Evidence\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| $<1.0$ | $< 3:1$ | $<0.750$ | need better data |\n",
    "| $<2.5$ | $< 12:1$ | $0.923$ | weak |\n",
    "| $<5.0$ | $< 150:1$ | $0.993$ | moderate |\n",
    "| $>5.0$ | $> 150:1$ | $> 0.993$ | strong |\n",
    "\n",
    "Evaluated according to the logarithm as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln{B_{12}} = \\ln \\frac{p (d | M_1)}{p (d | M_2)} = \\ln{p (d | M_1)} - \\ln{p (d | M_2)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The Bayes factor gives a __compromise between quality of fit and additional model complexity__ (such as additional free parameters). This means that it will \"reward\" highly predictive models, \"penalising\" unnecessary extra parameter space. This is what is often referred to as _Occam's razor_."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "293px",
    "left": "1351px",
    "right": "20px",
    "top": "120px",
    "width": "309px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
